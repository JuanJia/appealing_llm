{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from autogen import AssistantAgent, UserProxyAgent,GroupChat,GroupChatManager\n",
    "from autogen.coding import DockerCommandLineCodeExecutor\n",
    "import json\n",
    "from autogen.agentchat.contrib.capabilities import transform_messages, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give USER model name, opneai api key and base url here\n",
    "MODEL_NAME_USER = \"gpt-3.5-turbo\"\n",
    "OPENAI_API_KEY = \"sk-eb7c7b09A8\"\n",
    "OPENAI_BASE_URL_USER = \"https://api/v1\"\n",
    "config_list_user = [{\"model\": MODEL_NAME_USER,\"api_key\": OPENAI_API_KEY,\"base_url\": OPENAI_BASE_URL_USER}]\n",
    "\n",
    "# give EXAMINEE model name, opneai api key and base url here\n",
    "OPENAI_BASE_URL_EXAMINEE = \"http://127.0.0.1:8000/v1\"\n",
    "MODEL_NAME_EXAMINEE = \"RankingGPT-bloom-7b\"\n",
    "config_list_examinee = [{\"model\": MODEL_NAME_EXAMINEE,\"api_key\": OPENAI_API_KEY,\"base_url\": OPENAI_BASE_URL_EXAMINEE}]\n",
    "\n",
    "# OPENAI_BASE_URL_EXAMINEE = \"https://api/v1\"\n",
    "# MODEL_NAME_EXAMINEE = \"gpt-3.5-turbo\"\n",
    "# config_list_examinee = [{\"model\": MODEL_NAME_EXAMINEE,\"api_key\": OPENAI_API_KEY,\"base_url\": OPENAI_BASE_URL_EXAMINEE}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createChat(system_message_user,system_message_examinee,user_input_first):\n",
    "    # create an AssistantAgent instance named \"user_proxy\" with the LLM configuration to be a customer\n",
    "    user_proxy = UserProxyAgent(\n",
    "        name=\"customer\",\n",
    "        human_input_mode=\"NEVER\",\n",
    "        system_message=system_message_user,\n",
    "        llm_config={\n",
    "            \"cache_seed\": 32,  # seed 用户缓存中间结果\n",
    "            \"config_list\": config_list_user, \n",
    "            \"temperature\": 0,\n",
    "        },  \n",
    "        max_consecutive_auto_reply=10,\n",
    "        is_termination_msg=lambda msg: \"terminate\" in msg[\"content\"].lower(),\n",
    "        code_execution_config={\n",
    "            \"work_dir\": \"coding\",\n",
    "            \"use_docker\": False,  \n",
    "        },\n",
    "    )\n",
    "\n",
    "    # create an AssistantAgent instance named \"assistant_examinee\" with the LLM configuration to be a saler\n",
    "    assistant_examinee = AssistantAgent(\n",
    "        name=\"examinee\",\n",
    "        system_message=system_message_examinee,\n",
    "        is_termination_msg=lambda msg: \"terminate\" in msg[\"content\"].lower(),\n",
    "        llm_config={\n",
    "            \"cache_seed\": 42,  # seed 用户缓存中间结果\n",
    "            \"config_list\": config_list_examinee, \n",
    "            \"temperature\": 0,\n",
    "        },  \n",
    "    )\n",
    "\n",
    "    context_handling = transform_messages.TransformMessages(\n",
    "        transforms=[\n",
    "            transforms.MessageHistoryLimiter(max_messages=18),\n",
    "            transforms.MessageTokenLimiter(max_tokens=100, max_tokens_per_message=80, min_tokens=50),\n",
    "        ]\n",
    "    )\n",
    "    context_handling.add_to_agent(user_proxy)\n",
    "    context_handling.add_to_agent(assistant_examinee)\n",
    "\n",
    "    # the assistant receives a message from the user, which contains the task description\n",
    "    chatresult = user_proxy.initiate_chat(\n",
    "        assistant_examinee,\n",
    "        message=user_input_first,\n",
    "        max_turns=7,\n",
    "        silent = True\n",
    "    )\n",
    "    return chatresult\n",
    "    \n",
    "# save chat history \n",
    "def save_chat(chatresult, txt_path , scenario_eng=\"NONE\"):\n",
    "    chat_list = []\n",
    "    for chat in chatresult.chat_history:\n",
    "        chatter = 'assistant' if chat[\"role\"]=='user' else 'user'\n",
    "        content = chat[\"content\"].replace(\"\\n\", \" \")\n",
    "        line = f\"{chatter}: {content}\"\n",
    "        chat_list.append(line)\n",
    "    with open (txt_path, \"w\") as f:\n",
    "        f.write(\"scenario: \"+ scenario_eng + \"\\n\" + \"\\n\".join(chat_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_start(model_name,scene,folder_path_chat_history,folder_path_chat_scenes):\n",
    "    with open(f'{folder_path_chat_scenes}/{scene}.json', 'r') as f:\n",
    "        test_samples = json.load(f)\n",
    "    # test_samples = test_samples[\"scenarios\"]\n",
    "    for i, sample in enumerate(test_samples):\n",
    "        # if i >= 5:\n",
    "        #     break\n",
    "        # if i < 5:\n",
    "            # continue\n",
    "        txt_path = f\"{folder_path_chat_history}/chat_{model_name}_{scene}_{i}.txt\"\n",
    "        if not os.path.exists(txt_path):\n",
    "            try:\n",
    "                scenario_eng = sample['scenario_eng']\n",
    "                system_message_user = sample['system_message_user']\n",
    "                system_message_examinee = sample['system_message_examinee']\n",
    "                user_input_first = sample['user_input_first']\n",
    "                chatresult = createChat(system_message_user,system_message_examinee,user_input_first)\n",
    "                save_chat(chatresult,txt_path,scenario_eng)\n",
    "                print(f\"************{model_name}_{scene}_{i}******SUCCESSED******\")\n",
    "            except Exception as e:\n",
    "                print(f\"************{model_name}_{scene}_{i}******FAILED******\")\n",
    "                print(\"发生异常：\", str(e))\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenes = [\"saler\",\"contacting\",\"relationship\",\"streamer\",\"business\"]\n",
    "model_name = \"5\"\n",
    "folder_path_chat_scenes = \"./appeal_eval/chat_scenes\"\n",
    "folder_path_chat_history = f\"./appeal_eval/chat_history/chat_autogen_based/model_{model_name}\"\n",
    "\n",
    "if not os.path.exists(folder_path_chat_history):\n",
    "    os.makedirs(folder_path_chat_history)\n",
    "\n",
    "for scene in scenes:\n",
    "    print(f\"************{model_name}_{scene}************\")\n",
    "    chat_start(model_name,scene,folder_path_chat_history,folder_path_chat_scenes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyautogen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
